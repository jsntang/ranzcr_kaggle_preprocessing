{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,pprint,itertools,pprint\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('mdai20200531.json')) #this file not available as it contains private information of the labellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_map = {user['id']:user['email'] for user in data['users']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_labellers = ['redacted@gmail.com']\n",
    "assisted_labellers = ['redacted2@gmail.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id_to_label_group = {}\n",
    "label_id_to_name = {}\n",
    "label_id_to_tubetype = {}\n",
    "tubetype_to_label_name = {}\n",
    "label_name_to_tubetype = {}\n",
    "for i, label_group in enumerate(data['labelGroups']):\n",
    "    for label in label_group['labels']:\n",
    "        label_id_to_label_group[label['id']] = i\n",
    "        label_id_to_name[label['id']] = label['name']\n",
    "        label_type = label['name'].split(' ')[0].lower()\n",
    "        label_id_to_tubetype[label['id']] = label_type\n",
    "        label_name_to_tubetype[label['name']] = label_type\n",
    "        if label_type not in tubetype_to_label_name:\n",
    "            tubetype_to_label_name[label_type] = set()\n",
    "        tubetype_to_label_name[label_type].add(label['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tubetype_to_label_name = {k:sorted(list(v)) for k,v in tubetype_to_label_name.items()}\n",
    "tubetype_label_name_to_index = {k: {v:k for k,v in enumerate(label_names)} for k,label_names in tubetype_to_label_name.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dataset = data['datasets'][0] #use the first dataset always\n",
    "studyuid_to_examnum = {study['StudyInstanceUID']:study['number'] for study in dataset['studies']}\n",
    "examnum_to_studyuid = {study['number']:study['StudyInstanceUID'] for study in dataset['studies']}\n",
    "exam_num_to_index = {v:k for k,v in enumerate(list(set([studyuid_to_examnum[annotation['StudyInstanceUID']] for annotation in dataset['annotations']])))}\n",
    "index_to_exam_num = {k:v for k,v in enumerate(list(set([studyuid_to_examnum[annotation['StudyInstanceUID']] for annotation in dataset['annotations']])))}\n",
    "user_to_index = {v:k for k,v in enumerate([email for user_id,email in user_map.items() if email not in excluded_labellers])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_annotations = []\n",
    "for annotation in dataset['annotations']:\n",
    "    clean_annotations.append({\n",
    "        'exam_num':studyuid_to_examnum[annotation['StudyInstanceUID']],\n",
    "        'name': label_id_to_name[annotation['labelId']],\n",
    "        'labeller': user_map[annotation['createdById']],\n",
    "        'label_group': label_id_to_label_group[annotation['labelId']]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_annotations_dict = {'ett':defaultdict(list), 'ngt':defaultdict(list), 'cvc':defaultdict(list), 'swan':defaultdict(list), 'no':defaultdict(list)}\n",
    "gold_indices = set()\n",
    "for annotation in dataset['annotations']:\n",
    "    if label_id_to_label_group[annotation['labelId']] == 4:        \n",
    "        name = label_id_to_name[annotation['labelId']]\n",
    "        tubetype = label_name_to_tubetype[name]\n",
    "        examnum = studyuid_to_examnum[annotation['StudyInstanceUID']]\n",
    "        gold_annotations_dict[tubetype][exam_num_to_index[examnum]].append(tubetype_label_name_to_index[tubetype][name])        \n",
    "        gold_indices.add(exam_num_to_index[examnum])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tubetype in gold_annotations_dict:\n",
    "    for index in gold_indices:\n",
    "        if index not in gold_annotations_dict[tubetype]:\n",
    "            gold_annotations_dict[tubetype][index].append(len(tubetype_label_name_to_index[tubetype]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_per_case_per_labeller = {}\n",
    "for annotation in clean_annotations:\n",
    "    if annotation['labeller'] not in excluded_labellers:\n",
    "        if annotation['exam_num'] <= 30: #first 30 are demonstration cases \n",
    "            if annotation['labeller'] not in assisted_labellers:\n",
    "                continue\n",
    "        if annotation['exam_num'] not in labels_per_case_per_labeller:\n",
    "            labels_per_case_per_labeller[annotation['exam_num']] = {}\n",
    "        if annotation['label_group'] not in labels_per_case_per_labeller[annotation['exam_num']]:\n",
    "            labels_per_case_per_labeller[annotation['exam_num']][annotation['label_group']] = {}\n",
    "        if annotation['labeller'] not in labels_per_case_per_labeller[annotation['exam_num']][annotation['label_group']]:\n",
    "            labels_per_case_per_labeller[annotation['exam_num']][annotation['label_group']][annotation['labeller']] = set()\n",
    "            \n",
    "        labels_per_case_per_labeller[annotation['exam_num']][annotation['label_group']][annotation['labeller']].add(annotation['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_annotated = []\n",
    "for i in range(1,45000):\n",
    "    if i not in labels_per_case_per_labeller:\n",
    "        not_annotated.append(i)\n",
    "assert not len(not_annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for case in labels_per_case_per_labeller:\n",
    "    for label_group in labels_per_case_per_labeller[case]:        \n",
    "        annotators = list(labels_per_case_per_labeller[case][label_group].keys())\n",
    "        annotators_not_in_assisted = [a for a in annotators if a not in assisted_labellers]\n",
    "        annotators_in_assisted = [a for a in annotators if a in assisted_labellers]        \n",
    "        if len(annotators_not_in_assisted) > 0 and len(annotators_in_assisted) > 0:\n",
    "            chosen_annotator = annotators_not_in_assisted[0]\n",
    "            for annotator in annotators_in_assisted:\n",
    "                labels_per_case_per_labeller[case][label_group][chosen_annotator].update(labels_per_case_per_labeller[case][label_group][annotator])\n",
    "                del labels_per_case_per_labeller[case][label_group][annotator]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvc_annotations = np.zeros((len(exam_num_to_index), len(user_to_index), len(tubetype_to_label_name['cvc'])+1))\n",
    "ngt_annotations = np.zeros((len(exam_num_to_index), len(user_to_index), len(tubetype_to_label_name['ngt'])+1))\n",
    "ett_annotations = np.zeros((len(exam_num_to_index), len(user_to_index), len(tubetype_to_label_name['ett'])+1)) #add one for labeller saw the case and did not see a line of that type\n",
    "swan_annotations = np.zeros((len(exam_num_to_index), len(user_to_index), len(tubetype_to_label_name['swan'])+1)) #add one for labeller saw the case and did not see a line of that type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case in labels_per_case_per_labeller:\n",
    "    for label_group in labels_per_case_per_labeller[case]:       \n",
    "        for annotator in labels_per_case_per_labeller[case][label_group]:   \n",
    "            cvc_found = False\n",
    "            ngt_found = False\n",
    "            ett_found = False\n",
    "            swan_found= False\n",
    "            for label_name in labels_per_case_per_labeller[case][label_group][annotator]:                \n",
    "                if label_name_to_tubetype[label_name] == 'cvc':\n",
    "                    cvc_annotations[exam_num_to_index[case]][user_to_index[annotator]][tubetype_label_name_to_index['cvc'][label_name]] = 1\n",
    "                    cvc_found = True\n",
    "                elif label_name_to_tubetype[label_name] == 'ngt':\n",
    "                    ngt_annotations[exam_num_to_index[case]][user_to_index[annotator]][tubetype_label_name_to_index['ngt'][label_name]] = 1\n",
    "                    ngt_found = True\n",
    "                elif label_name_to_tubetype[label_name] == 'ett':\n",
    "                    ett_annotations[exam_num_to_index[case]][user_to_index[annotator]][tubetype_label_name_to_index['ett'][label_name]] = 1\n",
    "                    ett_found = True\n",
    "                elif label_name_to_tubetype[label_name] == 'swan':\n",
    "                    swan_annotations[exam_num_to_index[case]][user_to_index[annotator]][tubetype_label_name_to_index['swan']['Swan Ganz Catheter Present']] = 1\n",
    "                    cvc_annotations[exam_num_to_index[case]][user_to_index[annotator]][tubetype_label_name_to_index['cvc']['CVC - Normal']] = 1\n",
    "                    cvc_found = True\n",
    "                    swan_found = True\n",
    "            if not cvc_found:\n",
    "                cvc_annotations[exam_num_to_index[case]][user_to_index[annotator]][-1] = 1\n",
    "            if not ngt_found:\n",
    "                ngt_annotations[exam_num_to_index[case]][user_to_index[annotator]][-1] = 1\n",
    "            if not ett_found:\n",
    "                ett_annotations[exam_num_to_index[case]][user_to_index[annotator]][-1] = 1\n",
    "            if not swan_found:\n",
    "                swan_annotations[exam_num_to_index[case]][user_to_index[annotator]][-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_selections(indices,num_lines,forgotten_index):\n",
    "    '''\n",
    "    We collect binary yes / no answers for the presence or absence of each type of line\n",
    "    Since there can be more than one of each type of line present, if the number of unique indices \n",
    "    selected by the labeller is smaller than what we suspect the number of lines to be,\n",
    "    then we construct all possibilities of selection by adding duplicates of the selections\n",
    "    already made by that labeller, as well as accounting for the possibility that the labeller\n",
    "    may have simply forgotten to label a line, by adding the forgotten index, which is \n",
    "    simply the number of unique types of lines in that category. E.g. if there are \n",
    "    Type 0, Type 1, and Type 2 lines, then the index representing the labeller forgetting a line\n",
    "    is 3. \n",
    "    '''\n",
    "    assert len(set(indices)) == len(list(indices)), 'must have only unique indices!!'\n",
    "    selections = []\n",
    "    if len(indices) == num_lines: #they selected as many lines as there were, no need to add synthetic lines\n",
    "        selections.append(indices)\n",
    "    else:\n",
    "        difference = num_lines - len(indices)\n",
    "        for added_indices in itertools.combinations(indices + [forgotten_index], difference):\n",
    "            selections.append(indices+list(added_indices))\n",
    "    return selections\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert add_selections([0,1,2], 4, 3) == [[0, 1, 2, 0], [0, 1, 2, 1], [0, 1, 2, 2], [0, 1, 2, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert add_selections([0], 2, 3) == [[0, 0], [0, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert add_selections([0,2], 2, 3) == [[0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert add_selections([3], 2, 3) == [[3, 3], [3, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 1],\n",
       "       [1, 1, 0],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines = 3\n",
    "num_types_of_lines = 2\n",
    "np.stack(np.indices((num_types_of_lines,)*num_lines), -1).reshape((-1, num_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_prob_of_possibility(possibility, selection, labeller_error_rate):\n",
    "    return np.prod(labeller_error_rate[np.array(possibility), np.array(selection)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gold_standard_for_case(labeller_selections, labeller_error_rates, preselected_gold_standard=None, verbose=False):\n",
    "    '''\n",
    "    labeller_indices = [[0,1,2],[1,2] ...]    \n",
    "    '''\n",
    "    num_types_of_lines = labeller_error_rates[0].shape[-1] - 1\n",
    "    \n",
    "    if preselected_gold_standard: \n",
    "        preselected_gold_standard = list(set(preselected_gold_standard))\n",
    "        num_lines = max([len(indices) for indices in labeller_selections]) \n",
    "        if len(preselected_gold_standard) < num_lines:\n",
    "            preselected_gold_standard = list(preselected_gold_standard)\n",
    "            preselected_gold_standard.extend([num_types_of_lines]*(num_lines - len(preselected_gold_standard)))\n",
    "        num_lines = len(preselected_gold_standard)\n",
    "        assert len(preselected_gold_standard) >= max([len(indices) for indices in labeller_selections]) \n",
    "    else:\n",
    "        num_lines = max([len(indices) for indices in labeller_selections])        \n",
    "    \n",
    "    \n",
    "    \n",
    "    augmented_labeller_selection = []\n",
    "    # add in all the possible permutations to account for if someone forgot a line or if it was flipped the other way         \n",
    "    for indices in labeller_selections:\n",
    "        selections = add_selections(indices, num_lines, num_types_of_lines)\n",
    "        permuted_selections = []\n",
    "        for selection in selections:\n",
    "            permuted_selections.extend(itertools.permutations(selection))\n",
    "        augmented_labeller_selection.append(list(set(permuted_selections)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    all_labellers_selected_no_line = True\n",
    "    for indices in labeller_selections:\n",
    "        for index in indices:\n",
    "            if index != num_types_of_lines:\n",
    "                all_labellers_selected_no_line = False \n",
    "    \n",
    "    if preselected_gold_standard:\n",
    "        #since gold standrad is provided, only one possibility exists\n",
    "        possibilities = np.array([preselected_gold_standard])\n",
    "    else:\n",
    "        possibilities = list(np.stack(np.indices((num_types_of_lines,)*num_lines), -1).reshape((-1, num_lines)))\n",
    "        if num_lines == 1 and all_labellers_selected_no_line:\n",
    "            possibilities = [[num_types_of_lines,]]        \n",
    "        possibilities = np.array(possibilities)\n",
    "    \n",
    "    probabilities = []\n",
    "    best_selections_per_possibility = []\n",
    "    for possibility in possibilities:  \n",
    "        probability_of_this_possibility = 0    \n",
    "        best_selection_per_labeller = []\n",
    "        if verbose:\n",
    "            print ('Considering possibility', possibility)\n",
    "            print ('augmented labeller selection', augmented_labeller_selection)\n",
    "        for i, permuted_selections in enumerate(augmented_labeller_selection):\n",
    "            # assume permuted selections are all equally possible \n",
    "            labeller_error_rate = labeller_error_rates[i]\n",
    "            probability_per_selection = [check_prob_of_possibility(possibility, selection, labeller_error_rate) for selection in permuted_selections]\n",
    "            if verbose:\n",
    "                pprint.pprint (list(zip(permuted_selections, probability_per_selection)))\n",
    "            probability_of_this_possibility += np.max(probability_per_selection)\n",
    "            best_selection_per_labeller.append(permuted_selections[np.argmax(probability_per_selection)])\n",
    "        \n",
    "        best_selections_per_possibility.append(best_selection_per_labeller)\n",
    "        probabilities.append(probability_of_this_possibility+1E-10)\n",
    "    \n",
    "    if verbose:\n",
    "        print ('Final list of probabilities per possibility')\n",
    "        pprint.pprint (list(zip(possibilities,probabilities,best_selections_per_possibility)))\n",
    "    chosen_index = np.argmax(probabilities)\n",
    "    return possibilities[chosen_index], probabilities[chosen_index], best_selections_per_possibility[chosen_index]\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeller_error_rates = [np.array([[0.9,0.0,0.1],\n",
    "                                  [0.1,0.5,0.4],\n",
    "                                  [0.1,0.1,0.8],\n",
    "                                 ]),\n",
    "                        np.array([[0.9,0.0,0.1],\n",
    "                                  [0.0,0.9,0.1],\n",
    "                                  [0.1,0.1,0.8],\n",
    "                                 ])]\n",
    "labeller_indices = [[2],[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert find_gold_standard_for_case(labeller_indices, labeller_error_rates)[0] == np.array([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeller_error_rates = [np.array([[0.9,0.0,0.1],\n",
    "                                  [0.0,0.9,0.1],\n",
    "                                  [0.1,0.1,0.8],\n",
    "                                 ]),\n",
    "                        np.array([[0.9,0.0,0.1],\n",
    "                                  [0.0,0.9,0.1],\n",
    "                                  [0.1,0.1,0.8],\n",
    "                                 ])]\n",
    "labeller_indices = [[1],[0,1]] \n",
    "#Labeller 1 indicates that a single line of type 1 is present. Labeller 2 indicates that two lines, one of type 0 and one of type 1 are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_gold_standard_for_case(labeller_indices, labeller_error_rates)[0] \n",
    "#since both labeller 1 and labeller 2 are both pretty good at type 0 and type 1 lines, but can sometimes miss them with a 0.1 chance, algo thinks most likely labeller 1 simply missed a type 0 line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeller_error_rates = [np.array([[0.9,0.1,0.0],\n",
    "                                  [0.1,0.9,0.0],\n",
    "                                  [0.1,0.1,0.8],\n",
    "                                 ]),\n",
    "                        np.array([[0.8,0.2,0.0],\n",
    "                                  [0.2,0.8,0.0],\n",
    "                                  [0.1,0.1,0.8],\n",
    "                                 ])]\n",
    "find_gold_standard_for_case(labeller_indices, labeller_error_rates)[0] \n",
    "# now since labeller 2 is much weaker at identifying what type of line it is, and labeller 1 and 2 don't miss lines, the most likely explanation for the observed is that there are x2 type 1 lines. \n",
    "# labeller 1 observed x2 type 1 lines, but since we only annotate a binary yes/no\n",
    "# this means that labeller 1 would still indicate this as [1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_labeller(labeller, labeller_error_rates):\n",
    "    confusion_matrix = labeller_error_rates[user_to_index[labeller]]\n",
    "    return np.sum(np.eye(confusion_matrix.shape[0]) * confusion_matrix) / (confusion_matrix.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pprint\n",
    "def assess_annotation(target_annotations, predetermined_gold):\n",
    "    cases_more_than_one = np.where((target_annotations.sum(axis=2) > 0).sum(axis=1) > 2)[0]\n",
    "    annotations = target_annotations[cases_more_than_one]\n",
    "    # initialise labeller error rates\n",
    "    labeller_error_rates = np.array([np.eye(annotations.shape[-1]) for i in range(annotations.shape[1])])\n",
    "    max_iterations = 100\n",
    "    old_log_prob = 0\n",
    "    for num_iterations in tqdm(range(max_iterations)):\n",
    "        # Get gold standard\n",
    "        gold_standards = []\n",
    "        guessed_selections = []\n",
    "        labeller_indices_per_case = []\n",
    "        logprobs = []\n",
    "        for index, case in enumerate(annotations):\n",
    "            labeller_indices = np.where(case.sum(axis=1) > 0)[0]\n",
    "            labeller_selections = [list(np.where(case[labeller_index])[0]) for labeller_index in labeller_indices]\n",
    "            gold_standard, probability, guessed_selection = find_gold_standard_for_case(labeller_selections, \n",
    "                                                                labeller_error_rates[labeller_indices], \n",
    "                                                                preselected_gold_standard=(predetermined_gold[cases_more_than_one[index]] if cases_more_than_one[index] in predetermined_gold else None))\n",
    "            \n",
    "            gold_standards.append(gold_standard)\n",
    "            labeller_indices_per_case.append(labeller_indices)\n",
    "            guessed_selections.append(guessed_selection)   \n",
    "            logprobs.append(np.log(probability))\n",
    "        logprob = np.mean(logprobs)\n",
    "        print (np.mean(logprobs))\n",
    "        if np.abs(logprob - old_log_prob) < 1E-5:\n",
    "            break\n",
    "        old_log_prob = logprob\n",
    "        \n",
    "\n",
    "        # zero labeller error rates\n",
    "        for i, error_rate in enumerate(labeller_error_rates):\n",
    "            labeller_error_rates[i] = np.zeros((annotations.shape[-1], annotations.shape[-1]))\n",
    "        # Estimate labeller error rates\n",
    "        for i, gold_standard in enumerate(gold_standards):\n",
    "            for j, labeller_index in enumerate(labeller_indices_per_case[i]):\n",
    "                for k, gold_standard_label in enumerate(gold_standard):                \n",
    "                    labeller_error_rates[labeller_index][gold_standard_label, guessed_selections[i][j][k]] += 1\n",
    "        # normalize labeller error rates\n",
    "        for i, error_rate in enumerate(labeller_error_rates):\n",
    "            error_rate += np.ones(error_rate.shape[0]) * 1E-5\n",
    "            error_rate += np.eye(error_rate.shape[0]) * 1E-4\n",
    "            labeller_error_rates[i] = (error_rate) / (error_rate.sum(axis=0, keepdims=True) + 1E-10) \n",
    "    \n",
    "    # initialise zero labeller error rates    \n",
    "    final_labeller_error_rates = np.array([np.zeros((annotations.shape[-1], annotations.shape[-1])) for i in range(annotations.shape[1])])\n",
    "    \n",
    "    # Estimate labeller error rates\n",
    "    for i, gold_standard in enumerate(gold_standards):\n",
    "        for j, labeller_index in enumerate(labeller_indices_per_case[i]):\n",
    "            for k, gold_standard_label in enumerate(gold_standard):                \n",
    "                final_labeller_error_rates[labeller_index][gold_standard_label, guessed_selections[i][j][k]] += 1\n",
    "\n",
    "            \n",
    "    annotations = target_annotations\n",
    "    gold_standards = []\n",
    "    for index, case in enumerate(annotations):\n",
    "        labeller_indices = np.where(case.sum(axis=1) > 0)[0]\n",
    "        labeller_selections = [list(np.where(case[labeller_index])[0]) for labeller_index in labeller_indices]\n",
    "        \n",
    "        gold_standard, _, _ = find_gold_standard_for_case(labeller_selections, labeller_error_rates[labeller_indices], \n",
    "                                                          preselected_gold_standard=(predetermined_gold[index] if index in predetermined_gold else None))\n",
    "            \n",
    "            \n",
    "        gold_standards.append(gold_standard)\n",
    "    return [(index_to_exam_num[k], v) for k,v in enumerate(gold_standards)], final_labeller_error_rates, labeller_error_rates, sorted([(user,score_labeller(user, final_labeller_error_rates)) for user in user_to_index], key = lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba25fcb57734b2686fbff64ef80f09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9116252999530519\n",
      "0.9309535644704365\n",
      "0.9532738107429515\n",
      "0.9534350312360719\n",
      "0.9534357335459243\n"
     ]
    }
   ],
   "source": [
    "cvc_gold, cvc_error_rates, cvc_error_rates_2, cvc_scores = assess_annotation(cvc_annotations, predetermined_gold=gold_annotations_dict['cvc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5741051034f84b358ab69a51e3caa706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1654004149197565\n",
      "1.1441059859968443\n",
      "1.1443101996207992\n",
      "1.1443101996207992\n"
     ]
    }
   ],
   "source": [
    "ett_gold, ett_error_rates, ett_error_rates_2, ett_scores = assess_annotation(ett_annotations, predetermined_gold=gold_annotations_dict['ett'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70d1d0d7add477ab9326c251910b63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1494563982465555\n",
      "1.1014462984825482\n",
      "1.1041905279666655\n",
      "1.1041905279666655\n"
     ]
    }
   ],
   "source": [
    "ngt_gold, ngt_error_rates, ngt_error_rates_2, ngt_scores = assess_annotation(ngt_annotations, predetermined_gold=gold_annotations_dict['ngt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b861fc18b09f49d198bbd175ebf84587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2329633270687614\n",
      "1.2328287805850124\n",
      "1.2328287805850124\n"
     ]
    }
   ],
   "source": [
    "swan_gold, swan_error_rates, swan_error_rates_2,swan_scores = assess_annotation(swan_annotations, predetermined_gold=gold_annotations_dict['swan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4968, array([0]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvc_gold[exam_num_to_index[4968]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958, array([2]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ett_gold[exam_num_to_index[4958]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4964, array([3]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngt_gold[exam_num_to_index[4964]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ett': ['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal'],\n",
       " 'ngt': ['NGT - Abnormal',\n",
       "  'NGT - Borderline',\n",
       "  'NGT - Incompletely Imaged',\n",
       "  'NGT - Normal'],\n",
       " 'cvc': ['CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal'],\n",
       " 'no': ['No Label'],\n",
       " 'swan': ['Swan Ganz Catheter Present']}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tubetype_to_label_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae540dc44f1148578e7b8e02fd912248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=44999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8051ea6d5c44c4b97326fbb82f36d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=44999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a581f8502f2d47e4b72900ee403b6647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=44999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f613c3353f5b4842ad9fef040a83975e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=44999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "order = ['ett', 'ngt', 'cvc', 'swan']\n",
    "headers = ['StudyInstanceUID'] + [d for x in order for d in tubetype_to_label_name[x]]\n",
    "\n",
    "df = pd.DataFrame(columns=headers)\n",
    "df.set_index(['StudyInstanceUID'], inplace=True)\n",
    "\n",
    "for tubetype, gold in zip(order, [ett_gold, ngt_gold, cvc_gold, swan_gold]):\n",
    "    gold = dict(gold)\n",
    "    for exam_num in tqdm(range(1,45000)):\n",
    "        for i in gold[exam_num]:\n",
    "            if i < len(tubetype_to_label_name[tubetype]):\n",
    "                df.loc[examnum_to_studyuid[exam_num],tubetype_to_label_name[tubetype][i]] = 1\n",
    "df = df.fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('labels.csv.gz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
